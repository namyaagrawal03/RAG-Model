{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1AErTsUXWEdK0r0d6TyxhNYdcDe1xqMLr",
      "authorship_tag": "ABX9TyMCcZQeOKAad+EbyMCq0arg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/namyaagrawal03/Rag-Model/blob/main/rag_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index==0.10.12"
      ],
      "metadata": {
        "id": "zTF0yAdTn69c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "Aw06nT_-u8oJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a946cc79-f73b-4f09-93d5-8acfa4eddd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "3ORiVqgln9W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader,ServiceContext,PromptTemplate"
      ],
      "metadata": {
        "id": "SnQDt9syH76P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q accelerate"
      ],
      "metadata": {
        "id": "SZNZMpzbGhzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2E5v3skxm4Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import ServiceContext\n",
        "#from llama_index import LLMPredictor, PromptHelper\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core.text_splitter import TokenTextSplitter\n",
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "from llama_index.core import set_global_service_context\n",
        "from transformers import GPT2Tokenizer"
      ],
      "metadata": {
        "id": "ETt2ueTmmNyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "lotQ1d1qb2jL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n",
        "from IPython.display import Markdown , display"
      ],
      "metadata": {
        "id": "vestFqZae7Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-readers-file"
      ],
      "metadata": {
        "id": "tcYBW1OZxSd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import llama_index.core.readers as readers\n",
        "reader = readers.SimpleDirectoryReader(input_dir='/content/drive/MyDrive/RAG Model/data')\n",
        "documents1 = reader.load_data()"
      ],
      "metadata": {
        "id": "C1gV7vwWejju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho_lNgm3Wz-G",
        "outputId": "a38f2912-439f-4765-f937-6fd9ec731901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reader = readers.SimpleDirectoryReader(input_dir='/content/drive/MyDrive/RAG Model/data_docx')\n",
        "documents2 = reader.load_data()"
      ],
      "metadata": {
        "id": "LwMfCIOXWlux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents=documents1+ documents2\n"
      ],
      "metadata": {
        "id": "5q9i5QGAL9Bf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents"
      ],
      "metadata": {
        "id": "RpIHtDwmXxxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SimpleNodeParser\n",
        "# to create nodes or chunks\n",
        "# Create the SimpleNodeParser\n",
        "node_parser = SimpleNodeParser.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20\n",
        ")\n"
      ],
      "metadata": {
        "id": "UEaVrDWc-n2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "index=VectorStoreIndex.from_documents(documents)\n",
        "#create query engine (no memory component like chatbots)\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is Pradhan Mantri Jan Dan Yojana\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))\n",
        "'''"
      ],
      "metadata": {
        "id": "cf0aJBtJ8lQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Storing and loading the index"
      ],
      "metadata": {
        "id": "CgSgM2dBgBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#it saves any modification or updates made to the index storage context\n",
        "#its going to create a folder called storage:\n",
        "#storage folder contains:\n",
        "# vector store is basicaaly the embeddings that are computed for each chunk\n",
        "#docstore ,it tells about different chunks\n",
        "#indexstore, stores adress for different chunks, tells which embedding velongs to which chunk\n",
        "index=VectorStoreIndex.from_documents(documents)\n",
        "index.storage_context.persist()\n",
        "'''"
      ],
      "metadata": {
        "id": "FedR6fRzYZDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#storing and loading index\n",
        "from llama_index.core import StorageContext, load_index_from_storage\n",
        "#read content of index, used StorageContext\n",
        "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
        "#recreated index, like we did before it can be used like that\n",
        "index= load_index_from_storage(storage_context=storage_context)\n",
        "'''"
      ],
      "metadata": {
        "id": "K9x7CER7siMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customization of how to change the llm and chunk size\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JPlAmdDlgI8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI\n",
        "#define llm\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0, max_tokens=256)\n",
        "#define embedding model\n",
        "embed_model = OpenAIEmbedding()\n",
        "#configure service context\n",
        "#we are changing default llm to gpt 3.5\n",
        "\n",
        "service_context=ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=embed_model,\n",
        "    node_parser=node_parser)\n",
        "index=VectorStoreIndex.from_documents(documents, service_context=service_context)"
      ],
      "metadata": {
        "id": "2CGaOwlEKrZn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e488a0c-87b9-4326-c635-182a9a64cde3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-8c73b5b68624>:8: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context=ServiceContext.from_defaults(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is Pradhan Mantri Jan Dan Yojana\")\n",
        "display(Markdown(f\"<b>{response}</b>\"))"
      ],
      "metadata": {
        "id": "EP_oxwmik4bN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "2067e561-56a4-4b57-e936-314149dd61fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>Pradhan Mantri Jan-Dhan Yojana (PMJDY) is a National Mission for Financial Inclusion aimed at ensuring access to various financial services like basic savings bank accounts, credit, remittance, insurance, and pension for the excluded sections such as weaker sections and low-income groups. The plan focuses on universal access to banking facilities, financial literacy, and channeling government benefits to beneficiaries' accounts. Additionally, it provides benefits like no minimum balance requirement, interest on deposits, RuPay Debit card, accident insurance cover, overdraft facility, and eligibility for various government schemes.</b>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxkg3mFUFgdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}